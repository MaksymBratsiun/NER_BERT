{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "24685aa9-fa28-452f-b3f4-b04236bf3bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from transformers import BertTokenizer, BertForTokenClassification\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "012cac29-c690-47a8-a41b-102a3f164cb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "from torch import cuda\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "33f93688-2d71-4363-8f3a-d9ff3f97e5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "LABELS = [ \"O\", \"b-mount\", \"i-mount\"]\n",
    "num_labels_ = len(LABELS)\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "#model = BertForTokenClassification.from_pretrained(\"bert-base-uncased\", num_labels=num_labels_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "db0d4300-b9e0-4f40-9e22-79e02b8e49a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ya', '##yk', '##o', '-', 'pere', '##hin', '##ske']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(\"Yayko-Perehinske\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ace63ce3-14cf-4aa6-bfa2-c15ec629cd2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"Everest is the highest mountain in the world.\"\n",
    "sentence_tokenized = tokenizer.tokenize(sentence)\n",
    "text_labels = [\"b-mount\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\"]\n",
    "len(sentence_tokenized) == len(text_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a9437e73-2ef4-48ac-bd30-270862b1e349",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['everest', 'is', 'the', 'highest', 'mountain', 'in', 'the', 'world', '.'], ['b-mount', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'])\n"
     ]
    }
   ],
   "source": [
    "def tokenize_and_preserve_labels(sentence, text_labels, tokenizer):\n",
    "    tokenized_sentence = []\n",
    "    labels = []\n",
    "\n",
    "    for word, label in zip(sentence, text_labels):\n",
    "        \n",
    "        tokenized_word = tokenizer.tokenize(word)\n",
    "        n_subwords = len(tokenized_word)\n",
    "        tokenized_sentence.extend(tokenized_word)\n",
    "        labels.extend([label] * n_subwords)\n",
    "    if len(tokenized_sentence) == len(labels):\n",
    "        return tokenized_sentence, labels\n",
    "    \n",
    "a = tokenize_and_preserve_labels(sentence.split(), text_labels, tokenizer)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e351ef1f-01b7-4842-a680-89629d433669",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hoverla': 'b-mount',\n",
       " 'brebeneskul': 'b-mount',\n",
       " 'pip': 'b-mount',\n",
       " 'ivan': 'i-mount',\n",
       " 'petros': 'b-mount',\n",
       " 'hutyn': 'b-mount',\n",
       " 'tomnatyk': 'i-mount',\n",
       " 'rebra': 'b-mount',\n",
       " 'menchul': 'b-mount',\n",
       " 'turkul': 'b-mount',\n",
       " 'breskul': 'b-mount',\n",
       " 'smotrych': 'b-mount',\n",
       " 'blyznytsya': 'b-mount',\n",
       " 'dzembronia': 'b-mount',\n",
       " 'shpytsi': 'b-mount',\n",
       " 'petrosul': 'b-mount',\n",
       " 'dantsir': 'b-mount',\n",
       " 'pozhyzhevska': 'b-mount',\n",
       " 'neniska': 'b-mount',\n",
       " 'velyka': 'i-mount',\n",
       " 'syvulya': 'b-mount',\n",
       " 'ihrovets': 'b-mount',\n",
       " 'zherban': 'b-mount',\n",
       " 'bratkivska': 'b-mount',\n",
       " 'homul': 'b-mount',\n",
       " 'shuryn': 'b-mount',\n",
       " 'velyky': 'b-mount',\n",
       " 'kotel': 'i-mount',\n",
       " 'chyvchyn': 'b-mount',\n",
       " 'dohyaska': 'b-mount',\n",
       " 'hropa': 'b-mount',\n",
       " 'dragobrat': 'b-mount',\n",
       " 'dovbushanka': 'b-mount',\n",
       " 'grofa': 'b-mount',\n",
       " 'popadya': 'b-mount',\n",
       " 'parenky': 'b-mount',\n",
       " 'koman': 'b-mount',\n",
       " 'moloda': 'b-mount',\n",
       " 'strymba': 'b-mount',\n",
       " 'chorna': 'b-mount',\n",
       " 'kleva': 'i-mount',\n",
       " 'tataruka': 'b-mount',\n",
       " 'durna': 'b-mount',\n",
       " 'unharyaska': 'b-mount',\n",
       " 'nehrovets': 'b-mount',\n",
       " 'stih': 'b-mount',\n",
       " 'bushtul': 'b-mount',\n",
       " 'yayko': 'b-mount',\n",
       " 'ilemske': 'i-mount',\n",
       " 'budychevska': 'b-mount',\n",
       " 'stiy': 'b-mount',\n",
       " 'bert': 'b-mount',\n",
       " 'synyak': 'b-mount',\n",
       " 'tempa': 'b-mount',\n",
       " 'pidpula': 'b-mount',\n",
       " 'perehinske': 'i-mount',\n",
       " 'baba': 'b-mount',\n",
       " 'lyudova': 'i-mount',\n",
       " 'kernychny': 'b-mount',\n",
       " 'gorgan': 'b-mount',\n",
       " 'ilemsky': 'i-mount',\n",
       " 'skupova': 'b-mount',\n",
       " 'yarovytsya': 'b-mount',\n",
       " 'berlyaska': 'b-mount',\n",
       " 'tarnavytsya': 'b-mount',\n",
       " 'roztitska': 'b-mount',\n",
       " 'zhyd': 'b-mount',\n",
       " 'mahura': 'i-mount',\n",
       " 'verkh': 'i-mount',\n",
       " 'pikui': 'b-mount',\n",
       " 'parashka': 'b-mount',\n",
       " 'trostian': 'b-mount'}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_labels_dict(file):\n",
    "    names_labels = {} \n",
    "    with open(file, \"r\", encoding=\"utf-8\") as fd:\n",
    "        res = fd.readlines()\n",
    "        res = [i.strip() for i in res]\n",
    "        res = [i.removesuffix(\" \\n\") for i in res]\n",
    "        res = [i.removesuffix(\"\\n\") for i in res]\n",
    "    for name in res:\n",
    "        if len(name.split(\" \")) > 1:\n",
    "            subnames = name.split(\" \")\n",
    "            first_name = {subnames[0].lower(): \"b-mount\"}\n",
    "            names_labels.update(first_name)\n",
    "            second_name = {subnames[i].lower(): \"i-mount\" for i in range(1, len(subnames[1:]) + 1)}\n",
    "            names_labels.update(second_name)                        \n",
    "        elif len(name.split(\"-\")) > 1:\n",
    "            subnames = name.split(\"-\")\n",
    "            names_labels.update({subnames[0].lower(): \"b-mount\"})\n",
    "            names_labels.update({subnames[i].lower(): \"i-mount\" for i in range(1, len(subnames[1:]) + 1)})\n",
    "        else:\n",
    "            names_labels.update({name.lower(): \"b-mount\"})\n",
    "    return names_labels\n",
    "            \n",
    "labels_dict = create_labels_dict(\"ua_mountains.txt\")\n",
    "labels_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c62d5ad9-a53a-457c-b14d-4db9baf7f2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_punct(text, punct):\n",
    "    result = []\n",
    "    for i in list(text):\n",
    "        r = i.strip().split(punct)\n",
    "        if len(r) > 1:\n",
    "            for j in range(len(r)):\n",
    "                if j < len(r)-1:\n",
    "                    result.append(r[j] + \" \" + punct)\n",
    "                else:\n",
    "                    if r[j]: # last part without \"punct\", if it have \"punct\" in the end than last part will be \"\"\n",
    "                        result.append(r[j]) \n",
    "        else:\n",
    "            result.append(i)\n",
    "            \n",
    "    return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "11e4d428-cf7d-4aa6-a895-5fae17c2d415",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Together, Tempa, Pidpula, Yayko-Perehinske, Baba-Lyudova, Kernychny, Gorgan-Ilemsky, Skupova, Yarovytsya, Berlyaska, Tarnavytsya, and Roztitska form a captivating ensemble, inviting explorers to immerse themselves in the diverse wonders of the Ukrainian Carpathians .'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = \"ua_text.txt\"\n",
    "with open(file, \"r\", encoding=\"utf-8\") as fd:\n",
    "    res = []\n",
    "    while True:\n",
    "        line = fd.readline()\n",
    "        if not line:\n",
    "            break\n",
    "        res.append(line.replace(\"\\n\", \"\")) # yield to generator\n",
    "        \n",
    "res = split_punct(res, \"!\")\n",
    "res = split_punct(res, \"?\")\n",
    "res = split_punct(res, \".\")\n",
    "print(len(res))\n",
    "res[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "78175d7a-8043-4970-8274-4132652b1999",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "for sentence in res:\n",
    "    sentence_tokenized = tokenizer.tokenize(sentence.strip())\n",
    "    splited_sentense = []\n",
    "    word = []\n",
    "    for i in range(len(sentence_tokenized) - 1):\n",
    "        if i != 0:\n",
    "            if sentence_tokenized[i].startswith(\"##\"):\n",
    "                if not word:\n",
    "                    word.append(splited_sentense.pop())\n",
    "                    word.append(sentence_tokenized[i].removeprefix(\"##\"))\n",
    "                else:\n",
    "                    word.append(sentence_tokenized[i].removeprefix(\"##\"))\n",
    "            else:\n",
    "                if word:\n",
    "                    splited_sentense.append(\"\".join(word))\n",
    "                    word = []\n",
    "                    splited_sentense.append(sentence_tokenized[i])\n",
    "                else:\n",
    "                    splited_sentense.append(sentence_tokenized[i])\n",
    "        else:\n",
    "            splited_sentense.append(sentence_tokenized[0])\n",
    "    \n",
    "    masked_layer = []\n",
    "    for word in splited_sentense:\n",
    "        if word in labels_dict:\n",
    "            masked_layer.append(labels_dict[word])\n",
    "        else:\n",
    "            masked_layer.append(\"O\")\n",
    "    data.append(tokenize_and_preserve_labels(splited_sentense, masked_layer, tokenizer))\n",
    "    \n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "76720c45-07c2-43be-b2bf-0d7d348dd010",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'O': 0, 'b-mount': 1, 'i-mount': 2}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label2id = {k: v for v, k in enumerate(LABELS)}\n",
    "id2label = {v: k for v, k in enumerate(LABELS)}\n",
    "label2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c4f53d55-8b5d-4bdd-b6a6-8cca6b4e3f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 128\n",
    "TRAIN_BATCH_SIZE = 4\n",
    "VALID_BATCH_SIZE = 2\n",
    "EPOCHS = 5\n",
    "LEARNING_RATE = 1e-05\n",
    "MAX_GRAD_NORM = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fc68fd74-d595-495a-af41-9c1521a76fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [data[i][0] for i in range(len(data))]\n",
    "Y = [data[i][1] for i in range(len(data))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f3f88eb2-02e5-4c58-90f9-cf8655014d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b6a9c93f-94ad-4c1f-8141-5c9a4a948379",
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataset(Dataset):\n",
    "    def __init__(self, X, Y, tokenizer, max_len):\n",
    "        self.len = len(X)\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # step 1: tokenize (and adapt corresponding labels)\n",
    "        tokenized_sentence = self.X[index]\n",
    "        word_labels = self.Y[index]\n",
    "\n",
    "        # step 2: add special tokens (and corresponding labels)\n",
    "        tokenized_sentence = [\"[CLS]\"] + tokenized_sentence + [\"[SEP]\"] # add special tokens\n",
    "        word_labels = [\"O\"] + word_labels + [\"O\"] # add special tokens\n",
    "\n",
    "\n",
    "        # step 3: truncating/padding\n",
    "        maxlen = self.max_len\n",
    "\n",
    "        if (len(tokenized_sentence) > maxlen):\n",
    "          # truncate\n",
    "          tokenized_sentence = tokenized_sentence[:maxlen]\n",
    "          word_labels = word_labels[:maxlen]\n",
    "        else:\n",
    "          # pad\n",
    "          tokenized_sentence = tokenized_sentence + ['[PAD]'for _ in range(maxlen - len(tokenized_sentence))]\n",
    "          word_labels = word_labels + [\"O\" for _ in range(maxlen - len(word_labels))]\n",
    "\n",
    "        # step 4: obtain the attention mask\n",
    "        attn_mask = [1 if tok != '[PAD]' else 0 for tok in tokenized_sentence]\n",
    "\n",
    "        # step 5: convert tokens to input ids\n",
    "        ids = self.tokenizer.convert_tokens_to_ids(tokenized_sentence)\n",
    "\n",
    "        label_ids = [label2id[label] for label in word_labels]\n",
    "        # the following line is deprecated\n",
    "        #label_ids = [label if label != 0 else -100 for label in label_ids]\n",
    "\n",
    "        return {\n",
    "              'ids': torch.tensor(ids, dtype=torch.long),\n",
    "              'mask': torch.tensor(attn_mask, dtype=torch.long),\n",
    "              #'token_type_ids': torch.tensor(token_ids, dtype=torch.long),\n",
    "              'targets': torch.tensor(label_ids, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "05b00e34-118d-41e6-92ff-757f68169e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = dataset(X_train, y_train, tokenizer, MAX_LEN)\n",
    "testing_set = dataset(X_test, y_test, tokenizer, MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "62fdfc00-1b3f-4e2d-bc77-21cc854d2bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "test_params = {'batch_size': VALID_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "training_loader = DataLoader(training_set, **train_params)\n",
    "testing_loader = DataLoader(testing_set, **test_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9aec255a-fba0-4355-aca0-3ce233289ffc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForTokenClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BertForTokenClassification.from_pretrained('bert-base-uncased',\n",
    "                                                   num_labels=len(id2label),\n",
    "                                                   id2label=id2label,\n",
    "                                                   label2id=label2id)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e90fe3f6-8a60-4525-849c-64612582ac3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.1710, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids = training_set[0][\"ids\"].unsqueeze(0)\n",
    "mask = training_set[0][\"mask\"].unsqueeze(0)\n",
    "targets = training_set[0][\"targets\"].unsqueeze(0)\n",
    "ids = ids.to(device)\n",
    "mask = mask.to(device)\n",
    "targets = targets.to(device)\n",
    "outputs = model(input_ids=ids, attention_mask=mask, labels=targets)\n",
    "initial_loss = outputs[0]\n",
    "initial_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6c05ba50-97c5-4bcc-8ed0-876e6eebf2ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 128, 3])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_logits = outputs[1]\n",
    "tr_logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3dafb296-d070-4259-9b61-fe0094838569",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3fb93067-b4f3-4c00-90c6-a0f70fe1c0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    tr_loss, tr_accuracy = 0, 0\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "    tr_preds, tr_labels = [], []\n",
    "    # put model in training mode\n",
    "    model.train()\n",
    "\n",
    "    for idx, batch in enumerate(training_loader):\n",
    "\n",
    "        ids = batch['ids'].to(device, dtype = torch.long)\n",
    "        mask = batch['mask'].to(device, dtype = torch.long)\n",
    "        targets = batch['targets'].to(device, dtype = torch.long)\n",
    "\n",
    "        outputs = model(input_ids=ids, attention_mask=mask, labels=targets)\n",
    "        loss, tr_logits = outputs.loss, outputs.logits\n",
    "        tr_loss += loss.item()\n",
    "\n",
    "        nb_tr_steps += 1\n",
    "        nb_tr_examples += targets.size(0)\n",
    "\n",
    "        if idx % 100==0:\n",
    "            loss_step = tr_loss/nb_tr_steps\n",
    "            print(f\"Training loss per 100 training steps: {loss_step}\")\n",
    "\n",
    "        # compute training accuracy\n",
    "        flattened_targets = targets.view(-1) # shape (batch_size * seq_len,)\n",
    "        active_logits = tr_logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n",
    "        flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size * seq_len,)\n",
    "        # now, use mask to determine where we should compare predictions with targets (includes [CLS] and [SEP] token predictions)\n",
    "        active_accuracy = mask.view(-1) == 1 # active accuracy is also of shape (batch_size * seq_len,)\n",
    "        targets = torch.masked_select(flattened_targets, active_accuracy)\n",
    "        predictions = torch.masked_select(flattened_predictions, active_accuracy)\n",
    "\n",
    "        tr_preds.extend(predictions)\n",
    "        tr_labels.extend(targets)\n",
    "\n",
    "        tmp_tr_accuracy = accuracy_score(targets.cpu().numpy(), predictions.cpu().numpy())\n",
    "        tr_accuracy += tmp_tr_accuracy\n",
    "\n",
    "        # gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(\n",
    "            parameters=model.parameters(), max_norm=MAX_GRAD_NORM\n",
    "        )\n",
    "\n",
    "        # backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    epoch_loss = tr_loss / nb_tr_steps\n",
    "    tr_accuracy = tr_accuracy / nb_tr_steps\n",
    "    print(f\"Training loss epoch: {epoch_loss}\")\n",
    "    print(f\"Training accuracy epoch: {tr_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "353da68e-d0d1-407f-8245-83818dc1f73e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 1\n",
      "Training loss per 100 training steps: 1.1465680599212646\n",
      "Training loss epoch: 0.6939194947481155\n",
      "Training accuracy epoch: 0.6871337873990995\n",
      "Training epoch: 2\n",
      "Training loss per 100 training steps: 0.21100948750972748\n",
      "Training loss epoch: 0.16590232650438944\n",
      "Training accuracy epoch: 0.8720928151035992\n",
      "Training epoch: 3\n",
      "Training loss per 100 training steps: 0.09916052222251892\n",
      "Training loss epoch: 0.10192977078258991\n",
      "Training accuracy epoch: 0.8746521318636346\n",
      "Training epoch: 4\n",
      "Training loss per 100 training steps: 0.08047659695148468\n",
      "Training loss epoch: 0.06350787822157145\n",
      "Training accuracy epoch: 0.9374637863454932\n",
      "Training epoch: 5\n",
      "Training loss per 100 training steps: 0.0531141422688961\n",
      "Training loss epoch: 0.046093177516013384\n",
      "Training accuracy epoch: 0.9569449483395362\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    print(f\"Training epoch: {epoch + 1}\")\n",
    "    train(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d50d6e11-30ea-4269-9287-f9ce73773cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid(model, testing_loader):\n",
    "    # put model in evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_examples, nb_eval_steps = 0, 0\n",
    "    eval_preds, eval_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, batch in enumerate(testing_loader):\n",
    "\n",
    "            ids = batch['ids'].to(device, dtype = torch.long)\n",
    "            mask = batch['mask'].to(device, dtype = torch.long)\n",
    "            targets = batch['targets'].to(device, dtype = torch.long)\n",
    "\n",
    "            outputs = model(input_ids=ids, attention_mask=mask, labels=targets)\n",
    "            loss, eval_logits = outputs.loss, outputs.logits\n",
    "\n",
    "            eval_loss += loss.item()\n",
    "\n",
    "            nb_eval_steps += 1\n",
    "            nb_eval_examples += targets.size(0)\n",
    "\n",
    "            if idx % 100==0:\n",
    "                loss_step = eval_loss/nb_eval_steps\n",
    "                print(f\"Validation loss per 100 evaluation steps: {loss_step}\")\n",
    "\n",
    "            # compute evaluation accuracy\n",
    "            flattened_targets = targets.view(-1) # shape (batch_size * seq_len,)\n",
    "            active_logits = eval_logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n",
    "            flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size * seq_len,)\n",
    "            # now, use mask to determine where we should compare predictions with targets (includes [CLS] and [SEP] token predictions)\n",
    "            active_accuracy = mask.view(-1) == 1 # active accuracy is also of shape (batch_size * seq_len,)\n",
    "            targets = torch.masked_select(flattened_targets, active_accuracy)\n",
    "            predictions = torch.masked_select(flattened_predictions, active_accuracy)\n",
    "\n",
    "            eval_labels.extend(targets)\n",
    "            eval_preds.extend(predictions)\n",
    "\n",
    "            tmp_eval_accuracy = accuracy_score(targets.cpu().numpy(), predictions.cpu().numpy())\n",
    "            eval_accuracy += tmp_eval_accuracy\n",
    "\n",
    "    #print(eval_labels)\n",
    "    #print(eval_preds)\n",
    "\n",
    "    labels = [id2label[id.item()] for id in eval_labels]\n",
    "    predictions = [id2label[id.item()] for id in eval_preds]\n",
    "\n",
    "    #print(labels)\n",
    "    #print(predictions)\n",
    "\n",
    "    eval_loss = eval_loss / nb_eval_steps\n",
    "    eval_accuracy = eval_accuracy / nb_eval_steps\n",
    "    print(f\"Validation Loss: {eval_loss}\")\n",
    "    print(f\"Validation Accuracy: {eval_accuracy}\")\n",
    "\n",
    "    return labels, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "eed5744a-9a9e-444c-9a4e-63b8bcc8bc8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss per 100 evaluation steps: 0.02800721302628517\n",
      "Validation Loss: 0.02808769481877486\n",
      "Validation Accuracy: 0.9861345720720721\n"
     ]
    }
   ],
   "source": [
    "labels, predictions = valid(model, testing_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "18a5ed51-52f8-4926-ac2c-97c77202a6f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the highest mountain in ukraine is hoverla .\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'b-mount', 'b-mount', 'b-mount', 'O']\n"
     ]
    }
   ],
   "source": [
    "sentence = \" The highest mountain in Ukraine is Hoverla.\"\n",
    "\n",
    "inputs = tokenizer(sentence, padding='max_length', truncation=True, max_length=MAX_LEN, return_tensors=\"pt\")\n",
    "\n",
    "# move to gpu\n",
    "ids = inputs[\"input_ids\"].to(device)\n",
    "mask = inputs[\"attention_mask\"].to(device)\n",
    "# forward pass\n",
    "outputs = model(ids, mask)\n",
    "logits = outputs[0]\n",
    "\n",
    "active_logits = logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n",
    "flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size*seq_len,) - predictions at the token level\n",
    "\n",
    "tokens = tokenizer.convert_ids_to_tokens(ids.squeeze().tolist())\n",
    "token_predictions = [id2label[i] for i in flattened_predictions.cpu().numpy()]\n",
    "wp_preds = list(zip(tokens, token_predictions)) # list of tuples. Each tuple = (wordpiece, prediction)\n",
    "\n",
    "word_level_predictions = []\n",
    "for pair in wp_preds:\n",
    "  if (pair[0].startswith(\" ##\")) or (pair[0] in ['[CLS]', '[SEP]', '[PAD]']):\n",
    "    # skip prediction\n",
    "    continue\n",
    "  else:\n",
    "    word_level_predictions.append(pair[1])\n",
    "\n",
    "# we join tokens, if they are not special ones\n",
    "str_rep = \" \".join([t[0] for t in wp_preds if t[0] not in ['[CLS]', '[SEP]', '[PAD]']]).replace(\" ##\", \"\")\n",
    "print(str_rep)\n",
    "print(word_level_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e202405e-63c4-4fae-93b0-e07886035917",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, \"saved_model.save\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "79206eca-e3ee-4d99-90ad-f2b128f561bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForTokenClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = torch.load(\"saved_model.save\")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "415685fb-61d3-42a0-ae38-1264ea306e58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss per 100 evaluation steps: 0.017578210681676865\n",
      "Validation Loss: 0.028087694508334\n",
      "Validation Accuracy: 0.987349836184929\n"
     ]
    }
   ],
   "source": [
    "labels, predictions = valid(model, testing_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff63fb5-e18d-4cde-b08e-2cd20cf61ae5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
